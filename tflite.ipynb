{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import PIL\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, seg_path='FoliageFixerModel/models/mobilenetv2.3'):\n",
    "        super().__init__()\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "            print(torch.cuda.get_device_name(self.device))\n",
    "        else:\n",
    "            self.device = None\n",
    "            print('GPU is not available')\n",
    "        # Define your segmentation model here\n",
    "        # self.segmentation = models.segmentation.__dict__[\"fcn_resnet50\"](pretrained=True)\n",
    "        self.segmentation = torch.load(seg_path, map_location=torch.device('cpu'))\n",
    "        # Freeze the segmentation layers\n",
    "        for param in self.segmentation.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = self.device\n",
    "        # Forward pass through the segmentation model\n",
    "        # x = x/255.0\n",
    "        # resize\n",
    "        x = torchvision.transforms.Resize(size=(512,512))(x)\n",
    "        if x.shape[1] == 4:\n",
    "          # if batch_size is 1\n",
    "          if x.shape[0] == 1:\n",
    "            img = x.squeeze(0)\n",
    "            # transpose to shape: 512, 512, 4\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "            pil_image = PIL.Image.fromarray(img.numpy(), 'RGBA')\n",
    "            rgb_image = pil_image.convert('RGB')\n",
    "            rgb_array = np.asarray(rgb_image, dtype=np.float32)\n",
    "            x = torch.from_numpy(np.transpose(rgb_array, (2,1,0)))\n",
    "            x = x.unsqueeze(0)\n",
    "        x = x.to(device=device)\n",
    "        # segment image first\n",
    "        outputs = self.segmentation(x)\n",
    "        # Apply softmax activation function to the output\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        # Get the predicted labels\n",
    "        _, labels = torch.max(probs, dim=1)\n",
    "\n",
    "        disease_mask = (labels == 2).float()\n",
    "        disease_mask = torch.unsqueeze(disease_mask, 1)\n",
    "        healthy_mask = (labels == 1).float()\n",
    "        healthy_mask = torch.unsqueeze(healthy_mask, 1)\n",
    "        leaf_mask = disease_mask + healthy_mask\n",
    "\n",
    "        disease = x * disease_mask\n",
    "        leaf = x * leaf_mask\n",
    "        return (leaf, disease)\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super().__init__()\n",
    "        # Define your classification model here\n",
    "        self.classification = torchvision.models.resnet18(pretrained=True)\n",
    "        # Replace the last layer with a new layer that has num_classes outputs\n",
    "        num_features = self.classification.fc.in_features\n",
    "        self.classification.fc = nn.Linear(num_features, num_classes)\n",
    "        # self.label_dict = train_set.class_to_idx\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the classification model\n",
    "        x = self.classification(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassificationModel(\n",
       "  (classification): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Load segmentation model\n",
    "'''\n",
    "segmentation_model = SegmentationModel(seg_path='saved_seg_models/mobilenetv3')\n",
    "\n",
    "'''\n",
    "Load classification model\n",
    "'''\n",
    "classification_model = ClassificationModel()\n",
    "weights_path = 'saved_seg_models/classification-v5.1_stateDict'\n",
    "classification_model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n",
    "classification_model.train(mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 512 #model resizes anyway\n",
    "width = 512 #model resizes anyway\n",
    "sample_input_seg = torch.rand((batch_size, channels, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_10252/2343989904.py:23: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape[1] == 4:\n",
      "/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/segmentation_models_pytorch/base/model.py:16: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if h % output_stride != 0 or w % output_stride != 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = 'onnx_model/segmodelv3.onnx'\n",
    "\n",
    "torch.onnx.export(\n",
    "    segmentation_model,                  # PyTorch Model\n",
    "    sample_input_seg,                    # Input tensor\n",
    "    onnx_model_path,        # Output file (eg. 'output_model.onnx')\n",
    "    opset_version=12,       # Operator support version\n",
    "    input_names=['input'],   # Input tensor name (arbitary)\n",
    "    output_names=['output_leaf', 'output_disease'], # Output tensor name (arbitary)\n",
    "    # dynamic_axes={\"input\": {2: \"width\", 3: \"height\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 512 #model resizes anyway\n",
    "width = 512 #model resizes anyway\n",
    "sample_input_class = torch.rand((batch_size, channels, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = 'onnx_model/classmodelv5.1.onnx'\n",
    "\n",
    "torch.onnx.export(\n",
    "    classification_model,                  # PyTorch Model\n",
    "    sample_input_class,                    # Input tensor\n",
    "    onnx_model_path,        # Output file (eg. 'output_model.onnx')\n",
    "    opset_version=12,       # Operator support version\n",
    "    input_names=['input'],   # Input tensor name (arbitary)\n",
    "    output_names=['output'], # Output tensor name (arbitary)\n",
    "    # dynamic_axes={\"input\": {2: \"width\", 3: \"height\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph torch_jit (\\n  %input[FLOAT, 1x3x512x512]\\n) initializers (\\n  %classification.fc.weight[FLOAT, 8x512]\\n  %classification.fc.bias[FLOAT, 8]\\n  %onnx::Conv_193[FLOAT, 64x3x7x7]\\n  %onnx::Conv_194[FLOAT, 64]\\n  %onnx::Conv_196[FLOAT, 64x64x3x3]\\n  %onnx::Conv_197[FLOAT, 64]\\n  %onnx::Conv_199[FLOAT, 64x64x3x3]\\n  %onnx::Conv_200[FLOAT, 64]\\n  %onnx::Conv_202[FLOAT, 64x64x3x3]\\n  %onnx::Conv_203[FLOAT, 64]\\n  %onnx::Conv_205[FLOAT, 64x64x3x3]\\n  %onnx::Conv_206[FLOAT, 64]\\n  %onnx::Conv_208[FLOAT, 128x64x3x3]\\n  %onnx::Conv_209[FLOAT, 128]\\n  %onnx::Conv_211[FLOAT, 128x128x3x3]\\n  %onnx::Conv_212[FLOAT, 128]\\n  %onnx::Conv_214[FLOAT, 128x64x1x1]\\n  %onnx::Conv_215[FLOAT, 128]\\n  %onnx::Conv_217[FLOAT, 128x128x3x3]\\n  %onnx::Conv_218[FLOAT, 128]\\n  %onnx::Conv_220[FLOAT, 128x128x3x3]\\n  %onnx::Conv_221[FLOAT, 128]\\n  %onnx::Conv_223[FLOAT, 256x128x3x3]\\n  %onnx::Conv_224[FLOAT, 256]\\n  %onnx::Conv_226[FLOAT, 256x256x3x3]\\n  %onnx::Conv_227[FLOAT, 256]\\n  %onnx::Conv_229[FLOAT, 256x128x1x1]\\n  %onnx::Conv_230[FLOAT, 256]\\n  %onnx::Conv_232[FLOAT, 256x256x3x3]\\n  %onnx::Conv_233[FLOAT, 256]\\n  %onnx::Conv_235[FLOAT, 256x256x3x3]\\n  %onnx::Conv_236[FLOAT, 256]\\n  %onnx::Conv_238[FLOAT, 512x256x3x3]\\n  %onnx::Conv_239[FLOAT, 512]\\n  %onnx::Conv_241[FLOAT, 512x512x3x3]\\n  %onnx::Conv_242[FLOAT, 512]\\n  %onnx::Conv_244[FLOAT, 512x256x1x1]\\n  %onnx::Conv_245[FLOAT, 512]\\n  %onnx::Conv_247[FLOAT, 512x512x3x3]\\n  %onnx::Conv_248[FLOAT, 512]\\n  %onnx::Conv_250[FLOAT, 512x512x3x3]\\n  %onnx::Conv_251[FLOAT, 512]\\n) {\\n  %/classification/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [7, 7], pads = [3, 3, 3, 3], strides = [2, 2]](%input, %onnx::Conv_193, %onnx::Conv_194)\\n  %/classification/relu/Relu_output_0 = Relu(%/classification/conv1/Conv_output_0)\\n  %/classification/maxpool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/classification/relu/Relu_output_0)\\n  %/classification/layer1/layer1.0/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/maxpool/MaxPool_output_0, %onnx::Conv_196, %onnx::Conv_197)\\n  %/classification/layer1/layer1.0/relu/Relu_output_0 = Relu(%/classification/layer1/layer1.0/conv1/Conv_output_0)\\n  %/classification/layer1/layer1.0/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer1/layer1.0/relu/Relu_output_0, %onnx::Conv_199, %onnx::Conv_200)\\n  %/classification/layer1/layer1.0/Add_output_0 = Add(%/classification/layer1/layer1.0/conv2/Conv_output_0, %/classification/maxpool/MaxPool_output_0)\\n  %/classification/layer1/layer1.0/relu_1/Relu_output_0 = Relu(%/classification/layer1/layer1.0/Add_output_0)\\n  %/classification/layer1/layer1.1/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer1/layer1.0/relu_1/Relu_output_0, %onnx::Conv_202, %onnx::Conv_203)\\n  %/classification/layer1/layer1.1/relu/Relu_output_0 = Relu(%/classification/layer1/layer1.1/conv1/Conv_output_0)\\n  %/classification/layer1/layer1.1/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer1/layer1.1/relu/Relu_output_0, %onnx::Conv_205, %onnx::Conv_206)\\n  %/classification/layer1/layer1.1/Add_output_0 = Add(%/classification/layer1/layer1.1/conv2/Conv_output_0, %/classification/layer1/layer1.0/relu_1/Relu_output_0)\\n  %/classification/layer1/layer1.1/relu_1/Relu_output_0 = Relu(%/classification/layer1/layer1.1/Add_output_0)\\n  %/classification/layer2/layer2.0/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/classification/layer1/layer1.1/relu_1/Relu_output_0, %onnx::Conv_208, %onnx::Conv_209)\\n  %/classification/layer2/layer2.0/relu/Relu_output_0 = Relu(%/classification/layer2/layer2.0/conv1/Conv_output_0)\\n  %/classification/layer2/layer2.0/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer2/layer2.0/relu/Relu_output_0, %onnx::Conv_211, %onnx::Conv_212)\\n  %/classification/layer2/layer2.0/downsample/downsample.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [2, 2]](%/classification/layer1/layer1.1/relu_1/Relu_output_0, %onnx::Conv_214, %onnx::Conv_215)\\n  %/classification/layer2/layer2.0/Add_output_0 = Add(%/classification/layer2/layer2.0/conv2/Conv_output_0, %/classification/layer2/layer2.0/downsample/downsample.0/Conv_output_0)\\n  %/classification/layer2/layer2.0/relu_1/Relu_output_0 = Relu(%/classification/layer2/layer2.0/Add_output_0)\\n  %/classification/layer2/layer2.1/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer2/layer2.0/relu_1/Relu_output_0, %onnx::Conv_217, %onnx::Conv_218)\\n  %/classification/layer2/layer2.1/relu/Relu_output_0 = Relu(%/classification/layer2/layer2.1/conv1/Conv_output_0)\\n  %/classification/layer2/layer2.1/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer2/layer2.1/relu/Relu_output_0, %onnx::Conv_220, %onnx::Conv_221)\\n  %/classification/layer2/layer2.1/Add_output_0 = Add(%/classification/layer2/layer2.1/conv2/Conv_output_0, %/classification/layer2/layer2.0/relu_1/Relu_output_0)\\n  %/classification/layer2/layer2.1/relu_1/Relu_output_0 = Relu(%/classification/layer2/layer2.1/Add_output_0)\\n  %/classification/layer3/layer3.0/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/classification/layer2/layer2.1/relu_1/Relu_output_0, %onnx::Conv_223, %onnx::Conv_224)\\n  %/classification/layer3/layer3.0/relu/Relu_output_0 = Relu(%/classification/layer3/layer3.0/conv1/Conv_output_0)\\n  %/classification/layer3/layer3.0/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer3/layer3.0/relu/Relu_output_0, %onnx::Conv_226, %onnx::Conv_227)\\n  %/classification/layer3/layer3.0/downsample/downsample.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [2, 2]](%/classification/layer2/layer2.1/relu_1/Relu_output_0, %onnx::Conv_229, %onnx::Conv_230)\\n  %/classification/layer3/layer3.0/Add_output_0 = Add(%/classification/layer3/layer3.0/conv2/Conv_output_0, %/classification/layer3/layer3.0/downsample/downsample.0/Conv_output_0)\\n  %/classification/layer3/layer3.0/relu_1/Relu_output_0 = Relu(%/classification/layer3/layer3.0/Add_output_0)\\n  %/classification/layer3/layer3.1/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer3/layer3.0/relu_1/Relu_output_0, %onnx::Conv_232, %onnx::Conv_233)\\n  %/classification/layer3/layer3.1/relu/Relu_output_0 = Relu(%/classification/layer3/layer3.1/conv1/Conv_output_0)\\n  %/classification/layer3/layer3.1/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer3/layer3.1/relu/Relu_output_0, %onnx::Conv_235, %onnx::Conv_236)\\n  %/classification/layer3/layer3.1/Add_output_0 = Add(%/classification/layer3/layer3.1/conv2/Conv_output_0, %/classification/layer3/layer3.0/relu_1/Relu_output_0)\\n  %/classification/layer3/layer3.1/relu_1/Relu_output_0 = Relu(%/classification/layer3/layer3.1/Add_output_0)\\n  %/classification/layer4/layer4.0/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/classification/layer3/layer3.1/relu_1/Relu_output_0, %onnx::Conv_238, %onnx::Conv_239)\\n  %/classification/layer4/layer4.0/relu/Relu_output_0 = Relu(%/classification/layer4/layer4.0/conv1/Conv_output_0)\\n  %/classification/layer4/layer4.0/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer4/layer4.0/relu/Relu_output_0, %onnx::Conv_241, %onnx::Conv_242)\\n  %/classification/layer4/layer4.0/downsample/downsample.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [2, 2]](%/classification/layer3/layer3.1/relu_1/Relu_output_0, %onnx::Conv_244, %onnx::Conv_245)\\n  %/classification/layer4/layer4.0/Add_output_0 = Add(%/classification/layer4/layer4.0/conv2/Conv_output_0, %/classification/layer4/layer4.0/downsample/downsample.0/Conv_output_0)\\n  %/classification/layer4/layer4.0/relu_1/Relu_output_0 = Relu(%/classification/layer4/layer4.0/Add_output_0)\\n  %/classification/layer4/layer4.1/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer4/layer4.0/relu_1/Relu_output_0, %onnx::Conv_247, %onnx::Conv_248)\\n  %/classification/layer4/layer4.1/relu/Relu_output_0 = Relu(%/classification/layer4/layer4.1/conv1/Conv_output_0)\\n  %/classification/layer4/layer4.1/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/classification/layer4/layer4.1/relu/Relu_output_0, %onnx::Conv_250, %onnx::Conv_251)\\n  %/classification/layer4/layer4.1/Add_output_0 = Add(%/classification/layer4/layer4.1/conv2/Conv_output_0, %/classification/layer4/layer4.0/relu_1/Relu_output_0)\\n  %/classification/layer4/layer4.1/relu_1/Relu_output_0 = Relu(%/classification/layer4/layer4.1/Add_output_0)\\n  %/classification/avgpool/GlobalAveragePool_output_0 = GlobalAveragePool(%/classification/layer4/layer4.1/relu_1/Relu_output_0)\\n  %/classification/Flatten_output_0 = Flatten[axis = 1](%/classification/avgpool/GlobalAveragePool_output_0)\\n  %output = Gemm[alpha = 1, beta = 1, transB = 1](%/classification/Flatten_output_0, %classification.fc.weight, %classification.fc.bias)\\n  return %output\\n}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "model = onnx.load(\"onnx_model/classmodelv5.1.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a Human readable representation of the graph\n",
    "onnx.helper.printable_graph(model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"graph torch_jit (\\n  %input[FLOAT, 1x3x512x512]\\n) initializers (\\n  %segmentation.segmentation_head.0.weight[FLOAT, 3x16x3x3]\\n  %segmentation.segmentation_head.0.bias[FLOAT, 3]\\n  %onnx::Conv_825[FLOAT, 32x3x3x3]\\n  %onnx::Conv_826[FLOAT, 32]\\n  %onnx::Conv_828[FLOAT, 32x1x3x3]\\n  %onnx::Conv_829[FLOAT, 32]\\n  %onnx::Conv_831[FLOAT, 16x32x1x1]\\n  %onnx::Conv_832[FLOAT, 16]\\n  %onnx::Conv_834[FLOAT, 96x16x1x1]\\n  %onnx::Conv_835[FLOAT, 96]\\n  %onnx::Conv_837[FLOAT, 96x1x3x3]\\n  %onnx::Conv_838[FLOAT, 96]\\n  %onnx::Conv_840[FLOAT, 24x96x1x1]\\n  %onnx::Conv_841[FLOAT, 24]\\n  %onnx::Conv_843[FLOAT, 144x24x1x1]\\n  %onnx::Conv_844[FLOAT, 144]\\n  %onnx::Conv_846[FLOAT, 144x1x3x3]\\n  %onnx::Conv_847[FLOAT, 144]\\n  %onnx::Conv_849[FLOAT, 24x144x1x1]\\n  %onnx::Conv_850[FLOAT, 24]\\n  %onnx::Conv_852[FLOAT, 144x24x1x1]\\n  %onnx::Conv_853[FLOAT, 144]\\n  %onnx::Conv_855[FLOAT, 144x1x3x3]\\n  %onnx::Conv_856[FLOAT, 144]\\n  %onnx::Conv_858[FLOAT, 32x144x1x1]\\n  %onnx::Conv_859[FLOAT, 32]\\n  %onnx::Conv_861[FLOAT, 192x32x1x1]\\n  %onnx::Conv_862[FLOAT, 192]\\n  %onnx::Conv_864[FLOAT, 192x1x3x3]\\n  %onnx::Conv_865[FLOAT, 192]\\n  %onnx::Conv_867[FLOAT, 32x192x1x1]\\n  %onnx::Conv_868[FLOAT, 32]\\n  %onnx::Conv_870[FLOAT, 192x32x1x1]\\n  %onnx::Conv_871[FLOAT, 192]\\n  %onnx::Conv_873[FLOAT, 192x1x3x3]\\n  %onnx::Conv_874[FLOAT, 192]\\n  %onnx::Conv_876[FLOAT, 32x192x1x1]\\n  %onnx::Conv_877[FLOAT, 32]\\n  %onnx::Conv_879[FLOAT, 192x32x1x1]\\n  %onnx::Conv_880[FLOAT, 192]\\n  %onnx::Conv_882[FLOAT, 192x1x3x3]\\n  %onnx::Conv_883[FLOAT, 192]\\n  %onnx::Conv_885[FLOAT, 64x192x1x1]\\n  %onnx::Conv_886[FLOAT, 64]\\n  %onnx::Conv_888[FLOAT, 384x64x1x1]\\n  %onnx::Conv_889[FLOAT, 384]\\n  %onnx::Conv_891[FLOAT, 384x1x3x3]\\n  %onnx::Conv_892[FLOAT, 384]\\n  %onnx::Conv_894[FLOAT, 64x384x1x1]\\n  %onnx::Conv_895[FLOAT, 64]\\n  %onnx::Conv_897[FLOAT, 384x64x1x1]\\n  %onnx::Conv_898[FLOAT, 384]\\n  %onnx::Conv_900[FLOAT, 384x1x3x3]\\n  %onnx::Conv_901[FLOAT, 384]\\n  %onnx::Conv_903[FLOAT, 64x384x1x1]\\n  %onnx::Conv_904[FLOAT, 64]\\n  %onnx::Conv_906[FLOAT, 384x64x1x1]\\n  %onnx::Conv_907[FLOAT, 384]\\n  %onnx::Conv_909[FLOAT, 384x1x3x3]\\n  %onnx::Conv_910[FLOAT, 384]\\n  %onnx::Conv_912[FLOAT, 64x384x1x1]\\n  %onnx::Conv_913[FLOAT, 64]\\n  %onnx::Conv_915[FLOAT, 384x64x1x1]\\n  %onnx::Conv_916[FLOAT, 384]\\n  %onnx::Conv_918[FLOAT, 384x1x3x3]\\n  %onnx::Conv_919[FLOAT, 384]\\n  %onnx::Conv_921[FLOAT, 96x384x1x1]\\n  %onnx::Conv_922[FLOAT, 96]\\n  %onnx::Conv_924[FLOAT, 576x96x1x1]\\n  %onnx::Conv_925[FLOAT, 576]\\n  %onnx::Conv_927[FLOAT, 576x1x3x3]\\n  %onnx::Conv_928[FLOAT, 576]\\n  %onnx::Conv_930[FLOAT, 96x576x1x1]\\n  %onnx::Conv_931[FLOAT, 96]\\n  %onnx::Conv_933[FLOAT, 576x96x1x1]\\n  %onnx::Conv_934[FLOAT, 576]\\n  %onnx::Conv_936[FLOAT, 576x1x3x3]\\n  %onnx::Conv_937[FLOAT, 576]\\n  %onnx::Conv_939[FLOAT, 96x576x1x1]\\n  %onnx::Conv_940[FLOAT, 96]\\n  %onnx::Conv_942[FLOAT, 576x96x1x1]\\n  %onnx::Conv_943[FLOAT, 576]\\n  %onnx::Conv_945[FLOAT, 576x1x3x3]\\n  %onnx::Conv_946[FLOAT, 576]\\n  %onnx::Conv_948[FLOAT, 160x576x1x1]\\n  %onnx::Conv_949[FLOAT, 160]\\n  %onnx::Conv_951[FLOAT, 960x160x1x1]\\n  %onnx::Conv_952[FLOAT, 960]\\n  %onnx::Conv_954[FLOAT, 960x1x3x3]\\n  %onnx::Conv_955[FLOAT, 960]\\n  %onnx::Conv_957[FLOAT, 160x960x1x1]\\n  %onnx::Conv_958[FLOAT, 160]\\n  %onnx::Conv_960[FLOAT, 960x160x1x1]\\n  %onnx::Conv_961[FLOAT, 960]\\n  %onnx::Conv_963[FLOAT, 960x1x3x3]\\n  %onnx::Conv_964[FLOAT, 960]\\n  %onnx::Conv_966[FLOAT, 160x960x1x1]\\n  %onnx::Conv_967[FLOAT, 160]\\n  %onnx::Conv_969[FLOAT, 960x160x1x1]\\n  %onnx::Conv_970[FLOAT, 960]\\n  %onnx::Conv_972[FLOAT, 960x1x3x3]\\n  %onnx::Conv_973[FLOAT, 960]\\n  %onnx::Conv_975[FLOAT, 320x960x1x1]\\n  %onnx::Conv_976[FLOAT, 320]\\n  %onnx::Conv_978[FLOAT, 1280x320x1x1]\\n  %onnx::Conv_979[FLOAT, 1280]\\n  %onnx::Conv_981[FLOAT, 256x1376x3x3]\\n  %onnx::Conv_982[FLOAT, 256]\\n  %onnx::Conv_984[FLOAT, 256x256x3x3]\\n  %onnx::Conv_985[FLOAT, 256]\\n  %onnx::Conv_987[FLOAT, 32x128x3x3]\\n  %onnx::Conv_988[FLOAT, 32]\\n  %onnx::Conv_990[FLOAT, 32x32x3x3]\\n  %onnx::Conv_991[FLOAT, 32]\\n  %onnx::Conv_993[FLOAT, 24x56x3x3]\\n  %onnx::Conv_994[FLOAT, 24]\\n  %onnx::Conv_996[FLOAT, 24x24x3x3]\\n  %onnx::Conv_997[FLOAT, 24]\\n  %onnx::Conv_999[FLOAT, 16x40x3x3]\\n  %onnx::Conv_1000[FLOAT, 16]\\n  %onnx::Conv_1002[FLOAT, 16x16x3x3]\\n  %onnx::Conv_1003[FLOAT, 16]\\n  %onnx::Conv_1005[FLOAT, 128x320x3x3]\\n  %onnx::Conv_1006[FLOAT, 128]\\n  %onnx::Conv_1008[FLOAT, 128x128x3x3]\\n  %onnx::Conv_1009[FLOAT, 128]\\n  %onnx::Conv_1011[FLOAT, 24x80x3x3]\\n  %onnx::Conv_1012[FLOAT, 24]\\n  %onnx::Conv_1014[FLOAT, 24x24x3x3]\\n  %onnx::Conv_1015[FLOAT, 24]\\n  %onnx::Conv_1017[FLOAT, 16x56x3x3]\\n  %onnx::Conv_1018[FLOAT, 16]\\n  %onnx::Conv_1020[FLOAT, 16x16x3x3]\\n  %onnx::Conv_1021[FLOAT, 16]\\n  %onnx::Conv_1023[FLOAT, 64x200x3x3]\\n  %onnx::Conv_1024[FLOAT, 64]\\n  %onnx::Conv_1026[FLOAT, 64x64x3x3]\\n  %onnx::Conv_1027[FLOAT, 64]\\n  %onnx::Conv_1029[FLOAT, 16x72x3x3]\\n  %onnx::Conv_1030[FLOAT, 16]\\n  %onnx::Conv_1032[FLOAT, 16x16x3x3]\\n  %onnx::Conv_1033[FLOAT, 16]\\n  %onnx::Conv_1035[FLOAT, 32x128x3x3]\\n  %onnx::Conv_1036[FLOAT, 32]\\n  %onnx::Conv_1038[FLOAT, 32x32x3x3]\\n  %onnx::Conv_1039[FLOAT, 32]\\n  %onnx::Conv_1041[FLOAT, 16x32x3x3]\\n  %onnx::Conv_1042[FLOAT, 16]\\n  %onnx::Conv_1044[FLOAT, 16x16x3x3]\\n  %onnx::Conv_1045[FLOAT, 16]\\n) {\\n  %/Shape_output_0 = Shape(%input)\\n  %/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/Slice_output_0 = Slice(%/Shape_output_0, %/Constant_1_output_0, %/Constant_2_output_0, %/Constant_output_0)\\n  %/Constant_3_output_0 = Constant[value = <Tensor>]()\\n  %/Concat_output_0 = Concat[axis = 0](%/Slice_output_0, %/Constant_3_output_0)\\n  %/Constant_4_output_0 = Constant[value = <Tensor>]()\\n  %/Constant_5_output_0 = Constant[value = <Tensor>]()\\n  %/Resize_output_0 = Resize[coordinate_transformation_mode = 'half_pixel', cubic_coeff_a = -0.75, mode = 'linear', nearest_mode = 'floor'](%input, %/Constant_4_output_0, %/Constant_5_output_0, %/Concat_output_0)\\n  %/segmentation/encoder/features.0/features.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/Resize_output_0, %onnx::Conv_825, %onnx::Conv_826)\\n  %/segmentation/encoder/features.0/features.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.0/features.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.0/features.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.0/features.0.0/Conv_output_0, %/segmentation/encoder/features.0/features.0.2/Constant_output_0, %/segmentation/encoder/features.0/features.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.1/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.0/features.0.2/Clip_output_0, %onnx::Conv_828, %onnx::Conv_829)\\n  %/segmentation/encoder/features.1/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.1/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.1/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.1/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.1/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.1/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.1/conv/conv.1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.1/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_831, %onnx::Conv_832)\\n  %/segmentation/encoder/features.2/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.1/conv/conv.1/Conv_output_0, %onnx::Conv_834, %onnx::Conv_835)\\n  %/segmentation/encoder/features.2/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.2/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.2/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.2/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.2/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.2/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.2/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/segmentation/encoder/features.2/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_837, %onnx::Conv_838)\\n  %/segmentation/encoder/features.2/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.2/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.2/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.2/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.2/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.2/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.2/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.2/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_840, %onnx::Conv_841)\\n  %/segmentation/encoder/features.3/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.2/conv/conv.2/Conv_output_0, %onnx::Conv_843, %onnx::Conv_844)\\n  %/segmentation/encoder/features.3/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.3/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.3/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.3/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.3/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.3/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.3/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 144, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.3/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_846, %onnx::Conv_847)\\n  %/segmentation/encoder/features.3/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.3/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.3/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.3/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.3/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.3/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.3/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.3/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_849, %onnx::Conv_850)\\n  %/segmentation/encoder/features.3/Add_output_0 = Add(%/segmentation/encoder/features.2/conv/conv.2/Conv_output_0, %/segmentation/encoder/features.3/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.4/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.3/Add_output_0, %onnx::Conv_852, %onnx::Conv_853)\\n  %/segmentation/encoder/features.4/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.4/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.4/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.4/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.4/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.4/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.4/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 144, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/segmentation/encoder/features.4/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_855, %onnx::Conv_856)\\n  %/segmentation/encoder/features.4/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.4/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.4/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.4/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.4/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.4/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.4/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.4/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_858, %onnx::Conv_859)\\n  %/segmentation/encoder/features.5/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.4/conv/conv.2/Conv_output_0, %onnx::Conv_861, %onnx::Conv_862)\\n  %/segmentation/encoder/features.5/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.5/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.5/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.5/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.5/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.5/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.5/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.5/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_864, %onnx::Conv_865)\\n  %/segmentation/encoder/features.5/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.5/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.5/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.5/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.5/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.5/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.5/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.5/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_867, %onnx::Conv_868)\\n  %/segmentation/encoder/features.5/Add_output_0 = Add(%/segmentation/encoder/features.4/conv/conv.2/Conv_output_0, %/segmentation/encoder/features.5/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.6/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.5/Add_output_0, %onnx::Conv_870, %onnx::Conv_871)\\n  %/segmentation/encoder/features.6/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.6/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.6/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.6/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.6/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.6/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.6/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.6/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_873, %onnx::Conv_874)\\n  %/segmentation/encoder/features.6/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.6/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.6/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.6/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.6/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.6/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.6/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.6/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_876, %onnx::Conv_877)\\n  %/segmentation/encoder/features.6/Add_output_0 = Add(%/segmentation/encoder/features.5/Add_output_0, %/segmentation/encoder/features.6/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.7/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.6/Add_output_0, %onnx::Conv_879, %onnx::Conv_880)\\n  %/segmentation/encoder/features.7/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.7/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.7/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.7/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.7/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.7/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.7/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/segmentation/encoder/features.7/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_882, %onnx::Conv_883)\\n  %/segmentation/encoder/features.7/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.7/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.7/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.7/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.7/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.7/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.7/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.7/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_885, %onnx::Conv_886)\\n  %/segmentation/encoder/features.8/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.7/conv/conv.2/Conv_output_0, %onnx::Conv_888, %onnx::Conv_889)\\n  %/segmentation/encoder/features.8/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.8/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.8/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.8/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.8/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.8/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.8/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.8/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_891, %onnx::Conv_892)\\n  %/segmentation/encoder/features.8/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.8/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.8/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.8/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.8/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.8/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.8/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.8/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_894, %onnx::Conv_895)\\n  %/segmentation/encoder/features.8/Add_output_0 = Add(%/segmentation/encoder/features.7/conv/conv.2/Conv_output_0, %/segmentation/encoder/features.8/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.9/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.8/Add_output_0, %onnx::Conv_897, %onnx::Conv_898)\\n  %/segmentation/encoder/features.9/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.9/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.9/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.9/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.9/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.9/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.9/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.9/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_900, %onnx::Conv_901)\\n  %/segmentation/encoder/features.9/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.9/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.9/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.9/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.9/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.9/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.9/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.9/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_903, %onnx::Conv_904)\\n  %/segmentation/encoder/features.9/Add_output_0 = Add(%/segmentation/encoder/features.8/Add_output_0, %/segmentation/encoder/features.9/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.10/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.9/Add_output_0, %onnx::Conv_906, %onnx::Conv_907)\\n  %/segmentation/encoder/features.10/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.10/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.10/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.10/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.10/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.10/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.10/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.10/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_909, %onnx::Conv_910)\\n  %/segmentation/encoder/features.10/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.10/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.10/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.10/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.10/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.10/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.10/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.10/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_912, %onnx::Conv_913)\\n  %/segmentation/encoder/features.10/Add_output_0 = Add(%/segmentation/encoder/features.9/Add_output_0, %/segmentation/encoder/features.10/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.11/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.10/Add_output_0, %onnx::Conv_915, %onnx::Conv_916)\\n  %/segmentation/encoder/features.11/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.11/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.11/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.11/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.11/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.11/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.11/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.11/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_918, %onnx::Conv_919)\\n  %/segmentation/encoder/features.11/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.11/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.11/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.11/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.11/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.11/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.11/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.11/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_921, %onnx::Conv_922)\\n  %/segmentation/encoder/features.12/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.11/conv/conv.2/Conv_output_0, %onnx::Conv_924, %onnx::Conv_925)\\n  %/segmentation/encoder/features.12/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.12/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.12/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.12/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.12/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.12/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.12/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.12/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_927, %onnx::Conv_928)\\n  %/segmentation/encoder/features.12/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.12/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.12/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.12/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.12/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.12/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.12/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.12/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_930, %onnx::Conv_931)\\n  %/segmentation/encoder/features.12/Add_output_0 = Add(%/segmentation/encoder/features.11/conv/conv.2/Conv_output_0, %/segmentation/encoder/features.12/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.13/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.12/Add_output_0, %onnx::Conv_933, %onnx::Conv_934)\\n  %/segmentation/encoder/features.13/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.13/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.13/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.13/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.13/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.13/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.13/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.13/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_936, %onnx::Conv_937)\\n  %/segmentation/encoder/features.13/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.13/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.13/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.13/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.13/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.13/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.13/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.13/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_939, %onnx::Conv_940)\\n  %/segmentation/encoder/features.13/Add_output_0 = Add(%/segmentation/encoder/features.12/Add_output_0, %/segmentation/encoder/features.13/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.14/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.13/Add_output_0, %onnx::Conv_942, %onnx::Conv_943)\\n  %/segmentation/encoder/features.14/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.14/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.14/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.14/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.14/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.14/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.14/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/segmentation/encoder/features.14/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_945, %onnx::Conv_946)\\n  %/segmentation/encoder/features.14/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.14/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.14/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.14/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.14/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.14/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.14/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.14/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_948, %onnx::Conv_949)\\n  %/segmentation/encoder/features.15/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.14/conv/conv.2/Conv_output_0, %onnx::Conv_951, %onnx::Conv_952)\\n  %/segmentation/encoder/features.15/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.15/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.15/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.15/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.15/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.15/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.15/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.15/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_954, %onnx::Conv_955)\\n  %/segmentation/encoder/features.15/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.15/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.15/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.15/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.15/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.15/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.15/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.15/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_957, %onnx::Conv_958)\\n  %/segmentation/encoder/features.15/Add_output_0 = Add(%/segmentation/encoder/features.14/conv/conv.2/Conv_output_0, %/segmentation/encoder/features.15/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.16/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.15/Add_output_0, %onnx::Conv_960, %onnx::Conv_961)\\n  %/segmentation/encoder/features.16/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.16/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.16/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.16/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.16/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.16/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.16/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.16/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_963, %onnx::Conv_964)\\n  %/segmentation/encoder/features.16/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.16/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.16/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.16/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.16/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.16/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.16/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.16/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_966, %onnx::Conv_967)\\n  %/segmentation/encoder/features.16/Add_output_0 = Add(%/segmentation/encoder/features.15/Add_output_0, %/segmentation/encoder/features.16/conv/conv.2/Conv_output_0)\\n  %/segmentation/encoder/features.17/conv/conv.0/conv.0.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.16/Add_output_0, %onnx::Conv_969, %onnx::Conv_970)\\n  %/segmentation/encoder/features.17/conv/conv.0/conv.0.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.17/conv/conv.0/conv.0.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.17/conv/conv.0/conv.0.2/Clip_output_0 = Clip(%/segmentation/encoder/features.17/conv/conv.0/conv.0.0/Conv_output_0, %/segmentation/encoder/features.17/conv/conv.0/conv.0.2/Constant_output_0, %/segmentation/encoder/features.17/conv/conv.0/conv.0.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.17/conv/conv.1/conv.1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/encoder/features.17/conv/conv.0/conv.0.2/Clip_output_0, %onnx::Conv_972, %onnx::Conv_973)\\n  %/segmentation/encoder/features.17/conv/conv.1/conv.1.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.17/conv/conv.1/conv.1.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.17/conv/conv.1/conv.1.2/Clip_output_0 = Clip(%/segmentation/encoder/features.17/conv/conv.1/conv.1.0/Conv_output_0, %/segmentation/encoder/features.17/conv/conv.1/conv.1.2/Constant_output_0, %/segmentation/encoder/features.17/conv/conv.1/conv.1.2/Constant_1_output_0)\\n  %/segmentation/encoder/features.17/conv/conv.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.17/conv/conv.1/conv.1.2/Clip_output_0, %onnx::Conv_975, %onnx::Conv_976)\\n  %/segmentation/encoder/features.18/features.18.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/segmentation/encoder/features.17/conv/conv.2/Conv_output_0, %onnx::Conv_978, %onnx::Conv_979)\\n  %/segmentation/encoder/features.18/features.18.2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.18/features.18.2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/segmentation/encoder/features.18/features.18.2/Clip_output_0 = Clip(%/segmentation/encoder/features.18/features.18.0/Conv_output_0, %/segmentation/encoder/features.18/features.18.2/Constant_output_0, %/segmentation/encoder/features.18/features.18.2/Constant_1_output_0)\\n  %/segmentation/decoder/x_0_0/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_0/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_0/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/encoder/features.18/features.18.2/Clip_output_0, %/segmentation/decoder/x_0_0/Constant_1_output_0, %/segmentation/decoder/x_0_0/Constant_output_0)\\n  %/segmentation/decoder/x_0_0/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_0_0/Resize_output_0, %/segmentation/encoder/features.13/Add_output_0)\\n  %/segmentation/decoder/x_0_0/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_0/Concat_output_0, %onnx::Conv_981, %onnx::Conv_982)\\n  %/segmentation/decoder/x_0_0/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_0/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_0_0/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_0/conv1/conv1.2/Relu_output_0, %onnx::Conv_984, %onnx::Conv_985)\\n  %/segmentation/decoder/x_0_0/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_0/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_1_1/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_1_1/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_1_1/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/encoder/features.13/Add_output_0, %/segmentation/decoder/x_1_1/Constant_1_output_0, %/segmentation/decoder/x_1_1/Constant_output_0)\\n  %/segmentation/decoder/x_1_1/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_1_1/Resize_output_0, %/segmentation/encoder/features.6/Add_output_0)\\n  %/segmentation/decoder/x_1_1/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_1_1/Concat_output_0, %onnx::Conv_987, %onnx::Conv_988)\\n  %/segmentation/decoder/x_1_1/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_1_1/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_1_1/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_1_1/conv1/conv1.2/Relu_output_0, %onnx::Conv_990, %onnx::Conv_991)\\n  %/segmentation/decoder/x_1_1/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_1_1/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_2_2/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_2_2/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_2_2/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/encoder/features.6/Add_output_0, %/segmentation/decoder/x_2_2/Constant_1_output_0, %/segmentation/decoder/x_2_2/Constant_output_0)\\n  %/segmentation/decoder/x_2_2/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_2_2/Resize_output_0, %/segmentation/encoder/features.3/Add_output_0)\\n  %/segmentation/decoder/x_2_2/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_2_2/Concat_output_0, %onnx::Conv_993, %onnx::Conv_994)\\n  %/segmentation/decoder/x_2_2/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_2_2/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_2_2/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_2_2/conv1/conv1.2/Relu_output_0, %onnx::Conv_996, %onnx::Conv_997)\\n  %/segmentation/decoder/x_2_2/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_2_2/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_3_3/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_3_3/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_3_3/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/encoder/features.3/Add_output_0, %/segmentation/decoder/x_3_3/Constant_1_output_0, %/segmentation/decoder/x_3_3/Constant_output_0)\\n  %/segmentation/decoder/x_3_3/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_3_3/Resize_output_0, %/segmentation/encoder/features.1/conv/conv.1/Conv_output_0)\\n  %/segmentation/decoder/x_3_3/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_3_3/Concat_output_0, %onnx::Conv_999, %onnx::Conv_1000)\\n  %/segmentation/decoder/x_3_3/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_3_3/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_3_3/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_3_3/conv1/conv1.2/Relu_output_0, %onnx::Conv_1002, %onnx::Conv_1003)\\n  %/segmentation/decoder/x_3_3/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_3_3/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_0_1/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_1/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_1/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/decoder/x_0_0/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_0_1/Constant_1_output_0, %/segmentation/decoder/x_0_1/Constant_output_0)\\n  %/segmentation/decoder/x_0_1/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_0_1/Resize_output_0, %/segmentation/decoder/x_1_1/conv2/conv2.2/Relu_output_0, %/segmentation/encoder/features.6/Add_output_0)\\n  %/segmentation/decoder/x_0_1/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_1/Concat_output_0, %onnx::Conv_1005, %onnx::Conv_1006)\\n  %/segmentation/decoder/x_0_1/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_1/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_0_1/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_1/conv1/conv1.2/Relu_output_0, %onnx::Conv_1008, %onnx::Conv_1009)\\n  %/segmentation/decoder/x_0_1/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_1/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_1_2/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_1_2/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_1_2/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/decoder/x_1_1/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_1_2/Constant_1_output_0, %/segmentation/decoder/x_1_2/Constant_output_0)\\n  %/segmentation/decoder/x_1_2/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_1_2/Resize_output_0, %/segmentation/decoder/x_2_2/conv2/conv2.2/Relu_output_0, %/segmentation/encoder/features.3/Add_output_0)\\n  %/segmentation/decoder/x_1_2/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_1_2/Concat_output_0, %onnx::Conv_1011, %onnx::Conv_1012)\\n  %/segmentation/decoder/x_1_2/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_1_2/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_1_2/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_1_2/conv1/conv1.2/Relu_output_0, %onnx::Conv_1014, %onnx::Conv_1015)\\n  %/segmentation/decoder/x_1_2/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_1_2/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_2_3/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_2_3/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_2_3/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/decoder/x_2_2/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_2_3/Constant_1_output_0, %/segmentation/decoder/x_2_3/Constant_output_0)\\n  %/segmentation/decoder/x_2_3/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_2_3/Resize_output_0, %/segmentation/decoder/x_3_3/conv2/conv2.2/Relu_output_0, %/segmentation/encoder/features.1/conv/conv.1/Conv_output_0)\\n  %/segmentation/decoder/x_2_3/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_2_3/Concat_output_0, %onnx::Conv_1017, %onnx::Conv_1018)\\n  %/segmentation/decoder/x_2_3/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_2_3/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_2_3/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_2_3/conv1/conv1.2/Relu_output_0, %onnx::Conv_1020, %onnx::Conv_1021)\\n  %/segmentation/decoder/x_2_3/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_2_3/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_0_2/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_2/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_2/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/decoder/x_0_1/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_0_2/Constant_1_output_0, %/segmentation/decoder/x_0_2/Constant_output_0)\\n  %/segmentation/decoder/x_0_2/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_0_2/Resize_output_0, %/segmentation/decoder/x_1_2/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_2_2/conv2/conv2.2/Relu_output_0, %/segmentation/encoder/features.3/Add_output_0)\\n  %/segmentation/decoder/x_0_2/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_2/Concat_output_0, %onnx::Conv_1023, %onnx::Conv_1024)\\n  %/segmentation/decoder/x_0_2/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_2/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_0_2/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_2/conv1/conv1.2/Relu_output_0, %onnx::Conv_1026, %onnx::Conv_1027)\\n  %/segmentation/decoder/x_0_2/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_2/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_1_3/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_1_3/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_1_3/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/decoder/x_1_2/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_1_3/Constant_1_output_0, %/segmentation/decoder/x_1_3/Constant_output_0)\\n  %/segmentation/decoder/x_1_3/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_1_3/Resize_output_0, %/segmentation/decoder/x_2_3/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_3_3/conv2/conv2.2/Relu_output_0, %/segmentation/encoder/features.1/conv/conv.1/Conv_output_0)\\n  %/segmentation/decoder/x_1_3/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_1_3/Concat_output_0, %onnx::Conv_1029, %onnx::Conv_1030)\\n  %/segmentation/decoder/x_1_3/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_1_3/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_1_3/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_1_3/conv1/conv1.2/Relu_output_0, %onnx::Conv_1032, %onnx::Conv_1033)\\n  %/segmentation/decoder/x_1_3/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_1_3/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_0_3/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_3/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_3/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/decoder/x_0_2/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_0_3/Constant_1_output_0, %/segmentation/decoder/x_0_3/Constant_output_0)\\n  %/segmentation/decoder/x_0_3/Concat_output_0 = Concat[axis = 1](%/segmentation/decoder/x_0_3/Resize_output_0, %/segmentation/decoder/x_1_3/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_2_3/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_3_3/conv2/conv2.2/Relu_output_0, %/segmentation/encoder/features.1/conv/conv.1/Conv_output_0)\\n  %/segmentation/decoder/x_0_3/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_3/Concat_output_0, %onnx::Conv_1035, %onnx::Conv_1036)\\n  %/segmentation/decoder/x_0_3/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_3/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_0_3/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_3/conv1/conv1.2/Relu_output_0, %onnx::Conv_1038, %onnx::Conv_1039)\\n  %/segmentation/decoder/x_0_3/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_3/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/decoder/x_0_4/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_4/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/segmentation/decoder/x_0_4/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/segmentation/decoder/x_0_3/conv2/conv2.2/Relu_output_0, %/segmentation/decoder/x_0_4/Constant_1_output_0, %/segmentation/decoder/x_0_4/Constant_output_0)\\n  %/segmentation/decoder/x_0_4/conv1/conv1.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_4/Resize_output_0, %onnx::Conv_1041, %onnx::Conv_1042)\\n  %/segmentation/decoder/x_0_4/conv1/conv1.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_4/conv1/conv1.0/Conv_output_0)\\n  %/segmentation/decoder/x_0_4/conv2/conv2.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_4/conv1/conv1.2/Relu_output_0, %onnx::Conv_1044, %onnx::Conv_1045)\\n  %/segmentation/decoder/x_0_4/conv2/conv2.2/Relu_output_0 = Relu(%/segmentation/decoder/x_0_4/conv2/conv2.0/Conv_output_0)\\n  %/segmentation/segmentation_head/segmentation_head.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/segmentation/decoder/x_0_4/conv2/conv2.2/Relu_output_0, %segmentation.segmentation_head.0.weight, %segmentation.segmentation_head.0.bias)\\n  %/Transpose_output_0 = Transpose[perm = [0, 3, 2, 1]](%/segmentation/segmentation_head/segmentation_head.0/Conv_output_0)\\n  %/Softmax_output_0 = Softmax[axis = 3](%/Transpose_output_0)\\n  %/Transpose_1_output_0 = Transpose[perm = [0, 3, 2, 1]](%/Softmax_output_0)\\n  %/ArgMax_output_0 = ArgMax[axis = 1, keepdims = 0](%/Transpose_1_output_0)\\n  %/Constant_6_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/Equal_output_0 = Equal(%/ArgMax_output_0, %/Constant_6_output_0)\\n  %/Cast_output_0 = Cast[to = 1](%/Equal_output_0)\\n  %/Unsqueeze_output_0 = Unsqueeze[axes = [1]](%/Cast_output_0)\\n  %/Constant_7_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/Equal_1_output_0 = Equal(%/ArgMax_output_0, %/Constant_7_output_0)\\n  %/Cast_1_output_0 = Cast[to = 1](%/Equal_1_output_0)\\n  %/Unsqueeze_1_output_0 = Unsqueeze[axes = [1]](%/Cast_1_output_0)\\n  %/Add_output_0 = Add(%/Unsqueeze_output_0, %/Unsqueeze_1_output_0)\\n  %output_disease = Mul(%/Resize_output_0, %/Unsqueeze_output_0)\\n  %output_leaf = Mul(%/Resize_output_0, %/Add_output_0)\\n  return %output_leaf, %output_disease\\n}\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "model = onnx.load(\"onnx_model/segmodelv3.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a Human readable representation of the graph\n",
    "onnx.helper.printable_graph(model.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "seg_onnx_model = onnx.load('onnx_model/segmodelv3.onnx')\n",
    "class_onnx_model = onnx.load('onnx_model/classmodelv5.1.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 15:51:18.623182: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-08 15:51:19.831693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "\n",
    "seg_tf_rep = prepare(seg_onnx_model)\n",
    "class_tf_rep = prepare(class_onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/segmodelv3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/segmodelv3/assets\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/classmodelv5.1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/classmodelv5.1/assets\n",
      "INFO:absl:Writing fingerprint to tf_model/classmodelv5.1/fingerprint.pb\n"
     ]
    }
   ],
   "source": [
    "seg_tf_rep.export_graph('tf_model/segmodelv3')\n",
    "class_tf_rep.export_graph('tf_model/classmodelv5.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 15:51:54.005038: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-09-08 15:51:54.005104: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-09-08 15:51:54.005974: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: tf_model/segmodelv3\n",
      "2023-09-08 15:51:54.018332: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-09-08 15:51:54.018390: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: tf_model/segmodelv3\n",
      "2023-09-08 15:51:54.035874: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-09-08 15:51:54.040191: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-09-08 15:51:54.151737: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: tf_model/segmodelv3\n",
      "2023-09-08 15:51:54.214377: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 208405 microseconds.\n",
      "2023-09-08 15:51:54.368189: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-08 15:51:54.760237: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 35.801 G  ops, equivalently 17.901 G  MACs\n",
      "2023-09-08 15:51:56.003423: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-09-08 15:51:56.003480: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-09-08 15:51:56.003783: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: tf_model/classmodelv5.1\n",
      "2023-09-08 15:51:56.012537: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-09-08 15:51:56.012566: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: tf_model/classmodelv5.1\n",
      "2023-09-08 15:51:56.041386: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-09-08 15:51:56.135612: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: tf_model/classmodelv5.1\n",
      "2023-09-08 15:51:56.225701: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 221920 microseconds.\n",
      "2023-09-08 15:51:56.967542: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 18.976 G  ops, equivalently 9.488 G  MACs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('tf_model/segmodelv3')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('tf_lite/segmodelv3.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "    # Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('tf_model/classmodelv5.1')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('tf_lite/classmodelv5.1.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   3 512 512]\n",
      "[[ -7.0611405   1.625801    3.374012    3.253458  -20.40204    -3.9611185\n",
      "   -6.650133  -23.108    ]]\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "label_dict = {\n",
    "    'Bacterial Spot': 0, \n",
    "    'Early Blight': 1, \n",
    "    'Healthy': 2, \n",
    "    'Late Blight': 3,\n",
    "    'Leaf Mold': 4, \n",
    "    'Septoria Leaf Spot': 5, \n",
    "    'Tomato Mosaic Virus': 6, \n",
    "    'Yellow Leaf Curl Virus': 7\n",
    "    }\n",
    "\n",
    "def get_classification(outputs):\n",
    "    outputs = torch.from_numpy(outputs)\n",
    "    probabilities = torch.softmax(outputs, dim=1)\n",
    "    # this id may not be related to database ids\n",
    "    predicted_class_id = torch.argmax(probabilities, dim=1)\n",
    "    predicted_class = get_class_from_id(predicted_class_id.cpu().numpy()[0])\n",
    "    return predicted_class\n",
    "\n",
    "def get_class_from_id(id):\n",
    "  label = list(label_dict.keys())[list(label_dict.values()).index(id)]\n",
    "  return label\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tf_lite/classmodelv5.1.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "import torchvision\n",
    "input_data = torchvision.io.read_image('dataset-reduced/train/Healthy/0a0d6a11-ddd6-4dac-8469-d5f65af5afca___RS_HL 0555_flipTB.JPG')\n",
    "input_data = torchvision.transforms.functional.convert_image_dtype(input_data, dtype=torch.float32)\n",
    "input_data = torchvision.transforms.Resize((512,512))(input_data)\n",
    "input_data = input_data.unsqueeze(0)\n",
    "print(input_shape)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# get_tensor() returns a copy of the tensor data\n",
    "# use tensor() in order to get a pointer to the tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n",
    "\n",
    "pred = get_classification(output_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tf_lite/segmodelv3.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# get_tensor() returns a copy of the tensor data\n",
    "# use tensor() in order to get a pointer to the tensor\n",
    "leaf_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "disease_data = interpreter.get_tensor(output_details[1]['index'])\n",
    "\n",
    "print(leaf_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
